{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients & Policy Optimisation\n",
    "\n",
    "\n",
    "Up to now, we have been focused in **value-based RL**. This means, we learn the value function, and use the policy that is implicitly obtained from it. What we will do now is to turn it around, and go for **policy-based RL**: we just learn directly the policy, without the value function. A intermediate class of methods is called **Actor-Critic**, where both policy and value function are learnt. This later approach will not be discussed here.\n",
    "\n",
    "Why would this make sense? Well, sometimes we may just want to know what to do (left/right), not how bad or good it is to do either.\n",
    "\n",
    "So far, we have obtained policies through the $Q$-function (e.g. using $\\epsilon$-greedy), but we can parameterise directly the **policy**:\n",
    "\t$$ \\pi_\\theta(s,a) = \\mathbb P(a \\ | \\ s, \\theta) $$\n",
    "As with the value functions, if we use a differentiable parametrisation of the policy, we can update it by gradient descent.\n",
    "\n",
    "\n",
    "For example, we can assume the policy has the following structure:\n",
    "$$\\pi_\\theta(s,a) = \\frac{\\mathrm{exp}(h(s,a,\\theta))}{\\sum_{a'}\\mathrm{exp}(h(s,a',\\theta))}$$\n",
    "\n",
    "where \n",
    "\n",
    "$$h(s,a,\\theta) = \\theta^T\\cdot\\mathbf{x}(s,a)$$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The advantages of policy gradient are:\n",
    "* **Stronger convergence guarantees** (at least to a local optimum). \n",
    "* Effective in continuous action spaces.\n",
    "* No maximization of the $Q$ function is needed, which also means that the update is less \"aggressive\" than in $Q$-learning.\n",
    "\n",
    "\n",
    "\n",
    "### Objective function\n",
    "- **Goal**: find the best parameter $\\theta$ for $\\pi_\\theta(s,a)$, according a performance functional $J(\\theta)$.\n",
    "- Update according to the rule:\n",
    "$$ \\theta_{t+1} = \\theta_t + \\nabla \\hat{J}(\\theta_t)$$\n",
    "where $\\hat{J}$ is a stochastic approximation of the gradient (obtained, for example, by gradient **ascent**.\n",
    "\n",
    "\n",
    "## Derivative Free Methods\n",
    "\n",
    "Deep Learning's success relies on backpropagation and gradients. Gradients, however, are usually bad news for reinforcement learning for a number of reasons: two issues are that rewards come far in the future, so it is not obvious how to calculate derivatives, and the other issue is the existence of local optima.\n",
    "\n",
    "Luckily, there is a large class of optimization methods that do not use of the gradients (the expensive part of the computation), and are easy to parallelize. We will discuss two methods which have shown promising results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Entropy Method\n",
    "\n",
    "* Blackbox optimization for $f(w)$.\n",
    "* Start with $\\mu, \\sigma$, `batch_size`, `elite_frac`.\n",
    "* Do forever:\n",
    "  * Sample `batch_size` possible values for $w$.\n",
    "  * Evaluate $f$ on the sampled values.\n",
    "  * Keep the top `elite_frac` of those. These are `elite_set`.\n",
    "  * $\\mu \\leftarrow$ `np.mean(elite_set)`\n",
    "  * $\\sigma \\leftarrow$ `np.std(elite_set)`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Evolution Strategies\n",
    "\n",
    "It might not be good idea to take some elite solutions to resample and throw away the rest, since \"weak\" parameters are valuable because they tell us what *not* to do. An alternative approach from CEM is to maximise the expected value of the reward, so that when we would sample from that population, we would very likely get lucky and pick a good parameter.    \n",
    "\n",
    "[It has been shown](https://blog.openai.com/evolution-strategies/) that NES is comparable results to fancier algorithms in a number of tasks\n",
    "\n",
    "\n",
    "\n",
    "### NES Algorithm\n",
    "\n",
    "* Start with $\\sigma$, $\\alpha$, `n_estimators`.\n",
    "* Sample $\\epsilon_1, \\epsilon_2$ from a normal distribution with mean $0$ and variance $1$. These are **vectors**.\n",
    "* Calculate return $f(w+\\sigma \\cdot \\epsilon_i)$.\n",
    "*  $w \\leftarrow w + \\alpha\\cdot \\frac{1}{N}\\sum_{i=1}^n\\left(\\frac{f(w+\\sigma\\epsilon_i)-f(w)}{\\sigma}\\epsilon_i\\right)$ \n",
    "* Stopping criteria: function changes below a threshold, or, in the case of OpenAI Gym, performance benchmark is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Code sample: NES for FrozenLake\n",
    "import numpy as np\n",
    "import gym\n",
    "np.random.seed(0)\n",
    "\n",
    "env= gym.make('CartPole-v0')\n",
    "\n",
    "# the function we want to optimize\n",
    "def f(w):\n",
    "    n_iter = 200\n",
    "    total_reward = 0\n",
    "    for it in range(n_iter):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = 1 if np.matmul(state,w) < 0 else 0\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            total_reward+=reward\n",
    "            state = new_state\n",
    "    return total_reward/(1+it)\n",
    "\n",
    "# hyperparameters\n",
    "npop = 100 # population size\n",
    "sigma = 0.1 # noise standard deviation\n",
    "alpha = 0.001 # learning rate\n",
    "\n",
    "w = 2*np.random.random(4)-1 # our initial guess is random\n",
    "\n",
    "i = 0\n",
    "\n",
    "while f(w) <50:\n",
    "    i+=1\n",
    "  # print current fitness of the most likely parameter setting\n",
    "    if i % 5 == 0:\n",
    "        print('iter %d. w: %s, reward: %f' %(i, str(w),  f(w)))\n",
    "\n",
    "  # initialize memory for a population of w's, and their rewards\n",
    "    N = np.random.randn(npop, 4) # samples from a normal distribution N(0,1)\n",
    "    R = np.zeros(npop)\n",
    "    for j in range(npop):\n",
    "        w_try = w + sigma*N[j] # jitter jw using gaussian of sigma 0.1\n",
    "        R[j] = f(w_try) # evaluate the jittered version\n",
    "    \n",
    "    # standardize the rewards to have a gaussian distribution\n",
    "    A = (R - np.mean(R)) / np.std(R)\n",
    "    # perform the parameter update. The matrix multiply below\n",
    "    # is just an efficient way to sum up all the rows of the noise matrix N,\n",
    "    # where each row N[j] is weighted by A[j]\n",
    "    w = w + alpha/(npop*sigma+1) * np.dot(N.T, A)\n",
    "  \n",
    "env.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Code sample: Cross entropy method for CartPole\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# Task settings:\n",
    "env = gym.make('CartPole-v0') # Change as needed\n",
    "num_steps = 500 # maximum length of episode\n",
    "# Alg settings:\n",
    "n_iter = 100 # number of iterations of CEM\n",
    "batch_size = 25 # number of samples per batch\n",
    "elite_frac = 0.2 # fraction of samples used as elite set\n",
    "\n",
    "\n",
    "# Initialize mean and standard deviation\n",
    "dim_theta = (env.observation_space.shape[0]+1) * env.action_space.n\n",
    "theta_mean = np.zeros(dim_theta)\n",
    "theta_std = np.ones(dim_theta)\n",
    "\n",
    "\n",
    "def make_policy(theta):\n",
    "    n_states = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "    W = theta[0 : n_states * n_actions].reshape(n_states, n_actions)\n",
    "    b = theta[n_states * n_actions : None].reshape(1, n_actions)\n",
    "    \n",
    "    def policy_fn(state):\n",
    "        y = state.dot(W) + b\n",
    "        action = y.argmax()\n",
    "        return action\n",
    "        \n",
    "    return policy_fn\n",
    "\n",
    "def run_episode(theta, num_steps, render=False):\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    policy = make_policy(theta)\n",
    "    for t in range(num_steps):\n",
    "        a = policy(state)\n",
    "        state, reward, done, _ = env.step(a)\n",
    "        total_reward += reward\n",
    "        if render and t%3==0: env.render()\n",
    "        if done: break\n",
    "    return total_reward\n",
    "\n",
    "def noisy_evaluation(theta):\n",
    "    total_reward = run_episode(theta, num_steps)\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "# Now, for the algorithms\n",
    "for it in range(n_iter):\n",
    "    # Sample parameter vectors\n",
    "    thetas = np.random.normal(theta_mean, theta_std, (batch_size,dim_theta))\n",
    "    rewards = [noisy_evaluation(theta) for theta in thetas]\n",
    "    # Get elite parameters\n",
    "    n_elite = int(batch_size * elite_frac)\n",
    "    elite_inds = np.argsort(rewards)[batch_size - n_elite:batch_size]\n",
    "    elite_thetas = [thetas[i] for i in elite_inds]\n",
    "    # Update theta_mean, theta_std\n",
    "    theta_mean = np.mean(elite_thetas,axis=0)\n",
    "    theta_std = np.std(elite_thetas,axis=0)\n",
    "    #print(\"Mean reward f: {}. \\\n",
    "    # Max reward: {}\".format(np.mean(rewards), np.max(rewards)))\n",
    "    run_episode(theta_mean, num_steps, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CEM in steroids: DeepCEM\n",
    "\n",
    "Instead of generating the samples from a normal distribution, we can use an arbitrary (and unknown) distribution approximated by neural networks! Each step of the optimization generates a sample produced by a neural network and adjusts its weights based on the outcome. Example from [Yandex Data School](https://github.com/yandexdataschool/Practical_RL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class DeepCEM():\n",
    "    def __init__(self, initial_state, \\\n",
    "                 n_actions, \\\n",
    "                 clf=MLPClassifier(hidden_layer_sizes=(20,20), \\\n",
    "                                   activation='tanh', \\\n",
    "                                  warm_start=True, #keep progress between .fit(...) calls\n",
    "                                  max_iter=1 #make only 1 iteration on each .fit(...)\n",
    "                             )):\n",
    "        self.clf = clf\n",
    "        \n",
    "        #initialize clf\n",
    "        clf.fit([initial_state]*n_actions,range(n_actions))\n",
    "        \n",
    "    def policy(self, state):\n",
    "        probs = self.clf.predict_proba([state])[0]\n",
    "        action = np.random.choice(n_actions, p=probs)\n",
    "        return action\n",
    "\n",
    "    def train(self,n_iter):\n",
    "        n_episodes = 100 \n",
    "        percentile = 75\n",
    "        \n",
    "        for i in tqdm(range(n_iter)):\n",
    "            #generate new episodes\n",
    "            episodes = [play_episode(self) \\\n",
    "                        for _ in range(n_episodes)]\n",
    "\n",
    "            batch_states, batch_actions, batch_rewards = \\\n",
    "            map(np.array,zip(*episodes))\n",
    "\n",
    "            reward_threshold = np.percentile(batch_rewards,70)\n",
    "            idxs = [i for \\\n",
    "                    i in range(len(batch_rewards)) \\\n",
    "                    if batch_rewards[i]>reward_threshold]\n",
    "\n",
    "            elite_states, elite_actions = \\\n",
    "            np.concatenate(batch_states[idxs],axis=0), \\\n",
    "            np.concatenate(batch_actions[idxs],axis=0)\n",
    "            self.clf.fit(elite_states, elite_actions)\n",
    "\n",
    "            if np.mean(batch_rewards)> 195:\n",
    "                print(\"Solved.\")\n",
    "\n",
    "\n",
    "def play_episode(agent, max_iter=1000,render=False):\n",
    "    states,actions = [],[]\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    for _ in range(max_iter):\n",
    "        # choose the action according to the policy\n",
    "        action = agent.policy(state)\n",
    "        new_state,reward,done,info = env.step(action)\n",
    "        if render:\n",
    "            env.render()\n",
    "        # record sessions\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        total_reward+=reward\n",
    "        state = new_state\n",
    "        if done: \n",
    "            break\n",
    "    return states,actions,total_reward\n",
    "\n",
    "env = gym.make(\"CartPole-v0\").env  \n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "agent = DeepCEM(initial_state=env.reset(), n_actions=2)\n",
    "agent.train(n_iter=100)\n",
    "states, actions, rewards= play_episode(agent, render=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rlenv]",
   "language": "python",
   "name": "conda-env-rlenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
